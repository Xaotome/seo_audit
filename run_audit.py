#!/usr/bin/env python3
"""
Script de lancement simple pour l'outil d'audit SEO
"""

import sys
import os
import json
from pathlib import Path

# Ajouter le r√©pertoire courant au PATH Python
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def generate_web_files(engine, results, domain):
    """G√©n√®re les fichiers JSON pour l'interface web statique"""
    
    # Cr√©er le dossier web_data s'il n'existe pas
    web_data_dir = Path(__file__).parent / 'web_data'
    web_data_dir.mkdir(exist_ok=True)
    
    # G√©n√©rer les donn√©es pour l'interface web
    web_data = {
        'metadata': {
            'domain': domain,
            'analysis_date': results[0].crawled_at.isoformat() if results else None,
            'total_pages': len(results),
            'analysis_id': f"audit_{int(results[0].crawled_at.timestamp())}" if results else None
        },
        'summary': {
            'total_pages': engine.get_summary().total_pages,
            'pages_with_issues': engine.get_summary().pages_with_issues,
            'avg_response_time': engine.get_summary().avg_response_time,
            'total_issues': engine.get_summary().total_issues,
            'top_issues': engine.get_top_issues(10),
            'status_codes': dict(engine.get_summary().status_codes)
        },
        'pages': []
    }
    
    # Convertir les r√©sultats
    for result in results:
        page_data = {
            'url': result.url,
            'status': result.status,
            'responseTime': result.response_ms,
            'title': result.title,
            'titleLen': result.title_len,
            'metaDesc': result.meta_desc,
            'metaDescLen': result.meta_desc_len,
            'h1Count': result.h1_count,
            'canonical': result.canonical,
            'canonicalOk': result.canonical_ok,
            'robotsMeta': result.robots_meta,
            'noindex': result.noindex,
            'nofollow': result.nofollow,
            'imgNoAlt': result.img_no_alt,
            'linksInternal': result.links_internal,
            'linksExternal': result.links_external,
            'wordCount': result.word_count,
            'issues': result.issues,
            'issuesCount': len(result.issues),
            'redirectChain': result.redirect_chain,
            'hreflangCount': result.hreflang_count,
            'structuredDataCount': result.structured_data_count,
            'htmlSize': result.html_size,
            'isCompressed': result.is_compressed,
            'cacheHeaders': result.cache_headers,
            'crawledAt': result.crawled_at.isoformat(),
            # Heading structure
            'headingsStructure': [
                {
                    'level': h.level,
                    'text': h.text,
                    'position': h.position
                } for h in result.headings_structure
            ],
            'headingsHierarchyIssues': result.headings_hierarchy_issues
        }
        web_data['pages'].append(page_data)
    
    # Sauvegarder les donn√©es principales
    main_file = web_data_dir / 'latest_analysis.json'
    with open(main_file, 'w', encoding='utf-8') as f:
        json.dump(web_data, f, ensure_ascii=False, indent=2)
    
    # Sauvegarder l'historique
    history_file = web_data_dir / 'analysis_history.json'
    history = []
    if history_file.exists():
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except:
            history = []
    
    # Ajouter l'analyse actuelle
    history_entry = {
        'id': web_data['metadata']['analysis_id'],
        'domain': domain,
        'date': web_data['metadata']['analysis_date'],
        'summary': web_data['summary']
    }
    
    # Garder seulement les 20 derni√®res analyses
    history.insert(0, history_entry)
    history = history[:20]
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ Fichiers web g√©n√©r√©s dans {web_data_dir}")
    print(f"üìä {len(results)} pages analys√©es")
    print(f"üìÅ Fichier principal: {main_file}")

try:
    from seo_audit.cli import main
    from seo_audit.models import AuditConfig
    from seo_audit.audit_engine import SEOAuditEngine
    
    if __name__ == "__main__":
        # V√©rifier si c'est pour la g√©n√©ration web
        if len(sys.argv) > 1 and '--web-output' in sys.argv:
            # Mode g√©n√©ration pour interface web
            domain = None
            for arg in sys.argv[1:]:
                if arg.startswith('http'):
                    domain = arg
                    break
            
            if not domain:
                print("‚ùå Domaine requis pour la g√©n√©ration web")
                print("Usage: python3 run_audit.py https://example.com --web-output")
                sys.exit(1)
            
            # Configuration par d√©faut pour le web
            config = AuditConfig(
                domain=domain,
                max_pages=20,
                rate_limit=1.0,
                output_format='json'
            )
            
            # Cr√©er et lancer l'engine
            engine = SEOAuditEngine(config)
            
            if not engine.validate_config():
                sys.exit(1)
            
            print(f"üöÄ Analyse SEO pour interface web : {domain}")
            
            # Lancer l'analyse avec progression
            def progress_callback(current, total, url):
                percentage = (current / total) * 100
                print(f"[{current}/{total}] ({percentage:.1f}%) {url}")
            
            results = engine.run_audit(progress_callback)
            
            # G√©n√©rer les fichiers pour l'interface web
            generate_web_files(engine, results, domain)
            
            print("\nüåê Interface web pr√™te !")
            print("Ouvrez index.html dans votre navigateur")
            
        elif len(sys.argv) == 1:
            print("üîç Outil d'audit SEO")
            print("\nUsage:")
            print("  python3 run_audit.py <domain> [options]    # Analyse normale")
            print("  python3 run_audit.py <domain> --web-output # G√©n√©ration pour interface web")
            print("\nExemples:")
            print("  python3 run_audit.py https://httpbin.org")
            print("  python3 run_audit.py https://example.com --limit 50 --format json")
            print("  python3 run_audit.py https://example.com --web-output")
            print("\nPour voir toutes les options:")
            print("  python3 run_audit.py --help")
            sys.exit(0)
        else:
            # Mode CLI normal
            main()

except ImportError as e:
    print("‚ùå Erreur d'import:", e)
    print("\nüîß Installez d'abord les d√©pendances:")
    print("   python3 install.py")
    print("\nOu manuellement:")
    print("   pip3 install requests selectolax urllib3 pandas")
    sys.exit(1)